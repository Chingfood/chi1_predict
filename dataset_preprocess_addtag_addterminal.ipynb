{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = \"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/whole_seq/hhblits/train_scaled.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.load(train_scaled, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_dataset.shape[0]):\n",
    "    train_dataset[i] = np.insert(train_dataset[i],-1, np.zeros(train_dataset[i].shape[0]), axis = 1)\n",
    "\n",
    "    for j in range(-1,max(-15,-(train_dataset[i].shape[0])),-1):\n",
    "        \n",
    "        train_dataset[i][j,-2] = 1\n",
    "    train_dataset[i] = np.append(train_dataset[i],np.zeros([14,28]),axis = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/hhblits/train_tagged\",train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(train_dataset.shape[0]):\n",
    "    for j in range(train_dataset[i].shape[0]):\n",
    "        if train_dataset[i][j,-1] != 940501 and train_dataset[i][j,-1] != 930524 and train_dataset[i][j,-1] != 0 :\n",
    "            dataset.append(train_dataset[i][j:j+15])\n",
    "            \n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    np.save(\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/hhblits/train_scaled_\"+str(i),np.asarray(dataset[int(i*1e6):min(int((i+1)*1e6),len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = \"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/whole_seq/hhblits/test_scaled.npy\"\n",
    "test_dataset = np.load(test_scaled, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_dataset.shape[0]):\n",
    "    test_dataset[i] = np.insert(test_dataset[i],-1, np.zeros(test_dataset[i].shape[0]), axis = 1)\n",
    "\n",
    "    for j in range(-1,max(-15,-(test_dataset[i].shape[0])),-1):\n",
    "        \n",
    "        test_dataset[i][j,-2] = 1\n",
    "    test_dataset[i] = np.append(test_dataset[i],np.zeros([14,28]),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/hhblits/test_tagged\",test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = []\n",
    "for i in range(test_dataset.shape[0]):\n",
    "    for j in range(test_dataset[i].shape[0]):\n",
    "        if test_dataset[i][j,-1] != 940501 and test_dataset[i][j,-1] != 930524 and test_dataset[i][j,-1] != 0 :\n",
    "            dataset_test.append(test_dataset[i][j:j+15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.save(\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/hhblits/test_scaled_0\",np.asarray(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSSM_directory = \"/home/qingyliu/test\"#\"/home/chingyuenliu/Documents/test\"\n",
    "\n",
    "name_directory = \"/home/qingyliu/test\"#\"/home/chingyuenliu/Documents/test\"\n",
    "secondary_structure_directory = \"/home/qingyliu/test\"#\"/home/chingyuenliu/Documents/test\"#\n",
    "chi1_directory= \"/home/qingyliu/test\"#\"/home/chingyuenliu/Documents/test\"#\n",
    "chi1_native_distribution = \"/home/qingyliu/test/Chi1_native.pkl\"#\"/home/chingyuenliu/Documents/test/Chi1_native.pkl\"#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSSM_directory = \"/home/chingyuenliu/Documents/test\"\n",
    "\n",
    "name_directory = \"/home/chingyuenliu/Documents/test\"\n",
    "secondary_structure_directory = \"/home/chingyuenliu/Documents/test\"#\n",
    "chi1_directory= \"/home/chingyuenliu/Documents/test\"#\n",
    "chi1_native_distribution = \"/home/chingyuenliu/Documents/test/Chi1_native.pkl\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_convert(aa):\n",
    "    three_aa = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "    one_aa = {'C': 'CYS', 'D': 'ASP', 'S': 'SER', 'Q': 'GLN', 'K': 'LYS', 'I': 'ILE',\n",
    "    'P': 'PRO', 'T': 'THR', 'F': 'PHE', 'N': 'ASN',  'G': 'GLY',  'H': 'HIS',  'L': 'LEU',\n",
    "     'R': 'ARG', 'W': 'TRP', 'A': 'ALA', 'V': 'VAL', 'E': 'GLU', 'Y': 'TYR', 'M': 'MET'}\n",
    "    aa=aa.upper()\n",
    "    if len(aa) > 1:\n",
    "        try:\n",
    "            return three_aa[aa]\n",
    "        except:\n",
    "            if aa == \"SEC\" :\n",
    "                return 'C'\n",
    "            elif aa == \"PYL\":\n",
    "                return 'K'\n",
    "            else:\n",
    "                return \"X\"\n",
    "    else:\n",
    "        try: \n",
    "            return one_aa[aa]\n",
    "        except:\n",
    "            return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSSM_writing(dataset,filename,directory_path):\n",
    "    dataset.append([])\n",
    "    with open(os.path.join(directory_path,filename.replace('\\n','')+\".mtx\")) as f:\n",
    "        seq_len = int(f.readline())\n",
    "        seq = f.readline()\n",
    "        flag_1 = 2\n",
    "        for line in f:\n",
    "            flag_1 += 1\n",
    "            if line.startswith(\"-32768\"):\n",
    "                dataset[-1].append(np.zeros(20))\n",
    "                line = line.replace('\\n','').split()\n",
    "                flag = 0\n",
    "                for i in range(1,23):\n",
    "                    if i == 2 or i == 21:\n",
    "                        continue;\n",
    "                    else:\n",
    "                        try:\n",
    "                            dataset[-1][-1][flag] = float(line[i])\n",
    "                            flag += 1\n",
    "                        except:\n",
    "                            print(filename)\n",
    "                            print(flag_1)\n",
    "    return seq_len, seq\n",
    "                            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_structure_writing(dataset,filename,directory_path):\n",
    "    with open(os.path.join(directory_path,filename.replace('\\n','')+\".ss2\")) as f:\n",
    "        flag_2 = 0\n",
    "        \n",
    "        for i in range(2):\n",
    "            f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            dataset[-1][flag_2] = np.hstack((dataset[-1][flag_2],np.zeros(3)))\n",
    "            line = line.replace('\\n','').split()\n",
    "            for i in range(3):\n",
    "                dataset[-1][flag_2][20+i] = float(line[i+3])\n",
    "            flag_2 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def native_distribution(dataset,seq_len,seq,chi1_native = chi1_native_distribution):\n",
    "    f1 = open(chi1_native,\"rb\")\n",
    "    b = pickle.load(f1)\n",
    "    f1.close()\n",
    "    b['GLY'] = [1e-7,1e-7,1e-7]\n",
    "    b['ALA'] = [1e-7,1e-7,1e-7]\n",
    "    b['UNK'] = [0.96,0.03,0.01]\n",
    "    if (seq_len == len(dataset[-1])):\n",
    "        for i in range(seq_len):\n",
    "            dataset[-1][i] = np.hstack((dataset[-1][i],np.zeros(3)))\n",
    "            dataset[-1][i][-3:] = b[aa_convert(seq[i])]\n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_residue_fr(front_dataset):\n",
    "\n",
    "   \n",
    "    for j in range(len(front_dataset)):\n",
    "        for i in range(len(front_dataset[j])):\n",
    "            front_dataset[j][i] = np.insert(front_dataset[j][i],-1,0)\n",
    "\n",
    "\n",
    "            if i < 14:\n",
    "                front_dataset[j][i][-2] = 1\n",
    "\n",
    "    for j in range(len(front_dataset)):\n",
    "        for i in range(14):\n",
    "            front_dataset[j].insert(0,np.zeros(len(front_dataset[0][0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_residue_ba(back_dataset):\n",
    "\n",
    "    for j in range(len(back_dataset)):\n",
    "        for i in range(len(back_dataset[j])):\n",
    "            back_dataset[j][i] = np.insert(back_dataset[j][i],-1,0)\n",
    "    \n",
    "            if i > len(dataset[j]) - 15 :\n",
    "                back_dataset[j][i][-2] = 1\n",
    "    for j in range(len(dataset)):\n",
    "        for i in range(14):\n",
    "            \n",
    "            back_dataset[j].append(np.zeros(len(back_dataset[0][0])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def terminal_residue(dataset):\n",
    "    front_dataset = copy.deepcopy(dataset)\n",
    "    back_dataset = copy.deepcopy(dataset)\n",
    "    for j in range(len(dataset)):\n",
    "        for i in range(len(dataset[j])):\n",
    "            dataset[j][i] = np.insert(dataset[j][i],-1,0)\n",
    "            front_dataset[j][i] = np.insert(front_dataset[j][i],-1,0)\n",
    "            back_dataset[j][i] = np.insert(back_dataset[j][i],-1,0)\n",
    "            if i <7 or i >= len(dataset[j]) - 7:\n",
    "                dataset[j][i][-2] = 1\n",
    "            if i < 14:\n",
    "                front_dataset[j][i][-2] = 1\n",
    "            if i > len(dataset[j]) - 15 :\n",
    "                back_dataset[j][i][-2] = 1\n",
    "    for j in range(len(dataset)):\n",
    "        for i in range(14):\n",
    "            front_dataset[j].insert(0,np.zeros(len(front_dataset[0][0])))\n",
    "            back_dataset[j].append(np.zeros(len(back_dataset[0][0])))\n",
    "        for k in range(7):\n",
    "            dataset[j].insert(0,np.zeros(len(dataset[0][0])))\n",
    "            dataset[j].append(np.zeros(len(dataset[0][0])))\n",
    "    return front_dataset, back_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def terminal_residue(dataset):\n",
    "    for j in range(len(dataset)):\n",
    "        for i in range(len(dataset[j])):\n",
    "            dataset[j][i] = np.insert(dataset[j][i],-1,0)\n",
    "            if i <7 or i >= len(dataset[j]) - 7:\n",
    "                dataset[j][i][-2] = 1\n",
    "\n",
    "    for j in range(len(dataset)):\n",
    "        for k in range(7):\n",
    "            dataset[j].insert(0,np.zeros(len(dataset[0][0])))\n",
    "            dataset[j].append(np.zeros(len(dataset[0][0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi1_value(dataset,filename,directory_path,seq):\n",
    "    with open(os.path.join(directory_path,filename.replace('\\n',''))) as f:\n",
    "        flag_3 = 0\n",
    "        for i in range(len(dataset[-1])):\n",
    "            dataset[-1][i] = np.hstack((dataset[-1][i],np.zeros(1)))\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','').split()\n",
    "            if flag_3 < seq_len and aa_convert(aa_convert(seq[flag_3])) == aa_convert(line[0]):\n",
    "                if line[0] == \"ALA\" or line[0] == \"GLY\":\n",
    "                    dataset[-1][flag_3][-1] = 940501 #the residue is ala or gly\n",
    "                else:\n",
    "                    if (len(line) > 1):\n",
    "                        dataset[-1][flag_3][-1] = float(line[1])\n",
    "                    else:\n",
    "                        dataset[-1][flag_3][-1] = 930524 # the residue is missing coordinate to calculate chi1\n",
    "            else:\n",
    "                #print(\"Chi1 error not the same\")\n",
    "                #print(filename)\n",
    "                #print(flag_3+1)\n",
    "                continue\n",
    "            flag_3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "count =0\n",
    "with open(name_file_path) as f:\n",
    "    for line in f:\n",
    "        seq_len,seq = PSSM_writing(dataset,line.replace('\\n',''),PSSM_directory)\n",
    "        secondary_structure_writing(dataset,line.replace('\\n',''),secondary_structure_directory)\n",
    "        if (not native_distribution(dataset,seq_len,seq)):\n",
    "            print(\"native distribution\")\n",
    "            print(line)\n",
    "        \n",
    "        chi1_value(dataset,line.replace('\\n',''),chi1_directory,seq)  \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_residue_ba(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"back_dataset_1\",dataset[:11609])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"back_dataset_2\",dataset[11609:11609*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"back_dataset_3\",dataset[11609*2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e7a3323851e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mback_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminal_residue_ba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "back_dataset = terminal_residue_ba(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"middle_dataset_1\",data[:11609])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"middle_dataset_2\",data[11609:11609*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"middle_dataset_3\",data[11609*2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_dataset,back_dataset = terminal_residue_fr_ba(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"middle_dataset\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-746302323922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"middle_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"front_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfront_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"back_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mback_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 536\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.save(\"middle_dataset\", dataset)\n",
    "np.save(\"front_dataset\", front_dataset)\n",
    "np.save(\"back_dataset\",back_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"middle_dataset.pkl\",'wb')\n",
    "pickle.dump(dataset,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec3a2cf6decc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c8391d3cc997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"middle_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 536\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.save(\"middle_dataset\",b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
