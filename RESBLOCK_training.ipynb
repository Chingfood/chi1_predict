{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Orginal implementation from keras_contrib/layer/normalization\n",
    "# =============================================================================\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class GroupNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 groups=2,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super(GroupNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(\n",
    "            inputs, input_shape, tensor_input_shape)\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs,\n",
    "                                                      input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'groups':\n",
    "            self.groups,\n",
    "            'axis':\n",
    "            self.axis,\n",
    "            'epsilon':\n",
    "            self.epsilon,\n",
    "            'center':\n",
    "            self.center,\n",
    "            'scale':\n",
    "            self.scale,\n",
    "            'beta_initializer':\n",
    "            tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer':\n",
    "            tf.keras.initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer':\n",
    "            tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer':\n",
    "            tf.keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint':\n",
    "            tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint':\n",
    "            tf.keras.constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(GroupNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(\n",
    "            reshaped_inputs, group_reduction_axes, keepdims=True)\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon)\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                'Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                'more than the number of channels (' + str(dim) + ').')\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                'Number of groups (' + str(self.groups) + ') must be a '\n",
    "                'multiple of the number of channels (' + str(dim) + ').')\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\")\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            ndim=len(input_shape), axes={self.axis: dim})\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name='gamma',\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name='beta',\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape\n",
    "\n",
    "\n",
    "class InstanceNormalization(GroupNormalization):\n",
    "    \"\"\"Instance normalization layer.\n",
    "    Instance Normalization is an specific case of ```GroupNormalization```since\n",
    "    it normalizes all features of one channel. The Groupsize is equal to the\n",
    "    channel size. Empirically, its accuracy is more stable than batch norm in a\n",
    "    wide range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Arguments\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization]\n",
    "        (https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        if \"groups\" in kwargs:\n",
    "            logging.warning(\"The given value for groups will be overwritten.\")\n",
    "\n",
    "        kwargs[\"groups\"] = -1\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet_identity_block(X,kernelsize,num_filters,dilation):\n",
    "    X_shortcut = X\n",
    "    #First Component\n",
    "    X = keras.layers.Conv2D(filters = num_filters , kernel_size = kernelsize, padding = 'same', dilation_rate = dilation)(X)\n",
    "    X = InstanceNormalization()(X)\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "    X = keras.layers.Dropout(rate = 0.15)(X)\n",
    "    \n",
    "    \n",
    "    # Second component of main path\n",
    "    X = keras.layers.Conv2D(filters = num_filters , kernel_size = kernelsize, padding = 'same', dilation_rate = dilation)(X)\n",
    "    X = InstanceNormalization()(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = keras.layers.Add()([X, X_shortcut])\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def customLoss(yTrue,yPred):\n",
    "\n",
    "    if yTrue.shape[0] == None:\n",
    "        return 1e-6\n",
    "    \n",
    "    print(yPred)\n",
    "    yPred= tf.clip_by_value(yPred, 1e-6, (1. - 1e-6))\n",
    "    mask=K.less_equal(yTrue,2)\n",
    "    \n",
    "    return tf.reduce_mean(K.categorical_crossentropy(tf.one_hot(tf.cast(tf.boolean_mask(yTrue, mask),tf.int32), 3),tf.boolean_mask(yPred, mask)))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def crossentropy_cut(y_true,y_pred):\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    y_pred_f= tf.clip_by_value(y_pred_f, 1e-7, (1. - 1e-7))\n",
    "    mask=K.greater_equal(y_true_f,-0.5)\n",
    "    losses = -(y_true_f * K.log(y_pred_f) + (1.0 - y_true_f) * K.log(1.0 - y_pred_f))\n",
    "    losses = tf.boolean_mask(losses, mask)\n",
    "    masked_loss = tf.reduce_mean(losses)\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet():\n",
    "    X_input = keras.layers.Input(shape=(None,None,526))\n",
    "    X = keras.layers.Conv2D(filters = 64 , kernel_size = 1, padding = 'same')(X_input)\n",
    "    X = InstanceNormalization()(X)\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "    for i in range(36):\n",
    "        X=Resnet_identity_block(X,3,64,2**(i%5))\n",
    "    X = keras.layers.Conv2D(filters = 3 , kernel_size = 1, padding = 'same')(X)\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "\n",
    "    X_col = tf.reduce_mean(X,axis = 1)\n",
    "    X_row = tf.reduce_mean(X,axis = 2)\n",
    "    X = tf.math.add(X_col,X_row)\n",
    "    X = tf.math.multiply(X,0.5)\n",
    "\n",
    "    X = keras.layers.Conv1D(filters = 3 , kernel_size = 1, padding = 'same')(X)\n",
    "    X = keras.layers.Activation('softmax')(X)\n",
    "    model = keras.models.Model(inputs = X_input, outputs = X)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=customLoss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generator(input_feature_dir, input_label_dir,epochs):\n",
    "    directory = os.fsencode(input_feature_dir)\n",
    "    for j in range(epochs):\n",
    "        for file in os.listdir(directory):\n",
    "            \n",
    "            filename = os.fsdecode(file)\n",
    "            \n",
    "            yield np.load(os.path.join(input_feature_dir,filename)), np.load(os.path.join(input_label_dir,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def input_generator(input_feature_dir, input_label_dir,epochs):\n",
    "    directory = os.fsencode(input_feature_dir)\n",
    "    for j in range(epochs):\n",
    "        for file in os.listdir(directory):\n",
    "            \n",
    "            filename = \"1o13A.npy\"\n",
    "\n",
    "            \n",
    "            yield np.load(\"/nfs/amino-home/qingyliu/dihedral_angle/conv_ML_input/1o13A_nonan.npy\"), np.expand_dims(np.load(os.path.join(input_label_dir,filename)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_all_conv = \"/home/qingyliu/test/training_checkpoint/test\"\n",
    "checkpoint_dir_all_conv = os.path.dirname(checkpoint_path_all_conv)\n",
    "cp_callback_all_conv = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_all_conv,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_freq=2        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "tf.Tensor(\n",
      "[[[0.6768406  0.04110928 0.28205013]\n",
      "  [0.78910416 0.01712632 0.19376956]\n",
      "  [0.6619796  0.01623361 0.3217868 ]\n",
      "  [0.6575015  0.0252161  0.3172824 ]\n",
      "  [0.68477446 0.01065021 0.3045753 ]\n",
      "  [0.743647   0.00973902 0.24661396]\n",
      "  [0.73387784 0.00989666 0.25622547]\n",
      "  [0.7913956  0.0035379  0.20506646]\n",
      "  [0.68678033 0.01244625 0.30077344]\n",
      "  [0.6544598  0.00674373 0.33879656]\n",
      "  [0.6992179  0.01180179 0.28898025]\n",
      "  [0.6862218  0.01117802 0.30260023]\n",
      "  [0.72964835 0.00726885 0.2630828 ]\n",
      "  [0.7249161  0.02072339 0.25436047]\n",
      "  [0.8121087  0.00976084 0.17813043]\n",
      "  [0.770361   0.00741098 0.22222799]\n",
      "  [0.90673834 0.00356801 0.08969372]\n",
      "  [0.90367275 0.00896719 0.08736004]\n",
      "  [0.8894678  0.01292539 0.09760676]\n",
      "  [0.87953514 0.00783994 0.11262487]\n",
      "  [0.8875935  0.01816051 0.09424605]\n",
      "  [0.8898149  0.01055626 0.09962879]\n",
      "  [0.90672636 0.00598893 0.08728474]\n",
      "  [0.9084308  0.00384107 0.08772817]\n",
      "  [0.9125715  0.0029947  0.08443385]\n",
      "  [0.9261793  0.00467352 0.0691472 ]\n",
      "  [0.9113771  0.01032317 0.07829976]\n",
      "  [0.93184555 0.00479275 0.0633617 ]\n",
      "  [0.95356405 0.00509037 0.04134546]\n",
      "  [0.9250124  0.005067   0.0699206 ]\n",
      "  [0.9007096  0.00395094 0.09533948]\n",
      "  [0.92816246 0.00690045 0.0649371 ]\n",
      "  [0.9119471  0.00547588 0.08257712]\n",
      "  [0.9145648  0.0067296  0.07870568]\n",
      "  [0.9404889  0.00446883 0.05504234]\n",
      "  [0.948073   0.00570181 0.0462252 ]\n",
      "  [0.92534953 0.0052553  0.06939514]\n",
      "  [0.923459   0.00310482 0.0734362 ]\n",
      "  [0.92066246 0.00660566 0.07273193]\n",
      "  [0.91645    0.0070323  0.07651769]\n",
      "  [0.92358965 0.00498138 0.07142897]\n",
      "  [0.9129236  0.00495816 0.08211827]\n",
      "  [0.9129202  0.00381314 0.08326667]\n",
      "  [0.93336576 0.00362612 0.06300809]\n",
      "  [0.9308185  0.00327881 0.0659027 ]\n",
      "  [0.94726056 0.00457993 0.04815952]\n",
      "  [0.9181693  0.00398222 0.07784851]\n",
      "  [0.917839   0.00378592 0.07837508]\n",
      "  [0.9170595  0.00458422 0.07835629]\n",
      "  [0.9471845  0.00539048 0.04742501]\n",
      "  [0.95430154 0.00161761 0.04408095]\n",
      "  [0.9388293  0.00267475 0.05849602]\n",
      "  [0.9246721  0.00523603 0.07009181]\n",
      "  [0.9259678  0.00520644 0.06882577]\n",
      "  [0.9272957  0.007468   0.06523631]\n",
      "  [0.9364225  0.00262873 0.06094866]\n",
      "  [0.9129434  0.00622966 0.08082701]\n",
      "  [0.92516416 0.00447886 0.07035692]\n",
      "  [0.9522089  0.00221452 0.04557665]\n",
      "  [0.9332762  0.00961242 0.05711146]\n",
      "  [0.914772   0.00754895 0.07767914]\n",
      "  [0.93419576 0.0063483  0.05945593]\n",
      "  [0.9451584  0.00770557 0.04713602]\n",
      "  [0.951175   0.00583951 0.04298555]\n",
      "  [0.9224454  0.00938543 0.06816909]\n",
      "  [0.9356437  0.00306049 0.06129584]\n",
      "  [0.9516961  0.00419557 0.04410836]\n",
      "  [0.96515286 0.00121013 0.03363697]\n",
      "  [0.9492279  0.00362423 0.04714788]\n",
      "  [0.94826335 0.00285186 0.04888481]\n",
      "  [0.92957413 0.002841   0.06758482]\n",
      "  [0.92712957 0.00394194 0.06892857]\n",
      "  [0.91655296 0.00268485 0.08076219]\n",
      "  [0.9477446  0.00288052 0.0493748 ]\n",
      "  [0.93077904 0.0040696  0.06515138]\n",
      "  [0.965154   0.00157252 0.03327354]\n",
      "  [0.9524328  0.0020694  0.0454978 ]\n",
      "  [0.9665365  0.00168521 0.03177827]\n",
      "  [0.94344085 0.00648013 0.05007906]\n",
      "  [0.930394   0.00455733 0.06504864]\n",
      "  [0.96720904 0.00209149 0.03069947]\n",
      "  [0.9488017  0.00387116 0.04732712]\n",
      "  [0.9589881  0.00186319 0.03914877]\n",
      "  [0.95084083 0.00409976 0.04505943]\n",
      "  [0.9579711  0.00285957 0.03916937]\n",
      "  [0.96609527 0.00197724 0.0319276 ]\n",
      "  [0.94175655 0.00405017 0.05419318]\n",
      "  [0.9494978  0.005956   0.04454617]\n",
      "  [0.94159555 0.00662196 0.05178241]\n",
      "  [0.9492546  0.00469085 0.04605467]\n",
      "  [0.94812316 0.00383269 0.04804416]\n",
      "  [0.92377114 0.01099616 0.06523266]\n",
      "  [0.91655207 0.01452322 0.06892468]\n",
      "  [0.93672127 0.00721604 0.05606259]\n",
      "  [0.9196948  0.01128282 0.06902248]\n",
      "  [0.91124403 0.01448909 0.07426684]\n",
      "  [0.9207131  0.01473058 0.06455637]\n",
      "  [0.91399235 0.02362277 0.06238496]\n",
      "  [0.9219293  0.01572547 0.06234514]\n",
      "  [0.83419085 0.03984268 0.1259665 ]\n",
      "  [0.79911005 0.05507799 0.14581192]\n",
      "  [0.8720678  0.0219012  0.10603102]\n",
      "  [0.80113465 0.06790031 0.13096501]\n",
      "  [0.81705517 0.03831421 0.1446307 ]\n",
      "  [0.81040937 0.03285788 0.15673277]\n",
      "  [0.8751002  0.01630635 0.10859346]\n",
      "  [0.8499963  0.06351411 0.08648953]]], shape=(1, 107, 3), dtype=float32)\n",
      "WARNING:tensorflow:From /home/qingyliu/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "1/1 [==============================] - 8s 8s/step - loss: 3.1631\n",
      "Epoch 2/500\n",
      "tf.Tensor(\n",
      "[[[9.80776489e-01 4.04236062e-06 1.92193929e-02]\n",
      "  [9.92527366e-01 4.89988452e-07 7.47215981e-03]\n",
      "  [9.76886094e-01 6.02349883e-06 2.31078081e-02]\n",
      "  [9.79228139e-01 9.80964160e-06 2.07620673e-02]\n",
      "  [9.83177960e-01 1.46902621e-05 1.68074034e-02]\n",
      "  [9.85338330e-01 1.85581321e-05 1.46431420e-02]\n",
      "  [9.79460955e-01 4.90009079e-05 2.04900391e-02]\n",
      "  [9.86428440e-01 1.61960852e-05 1.35553218e-02]\n",
      "  [9.75867212e-01 2.23406532e-04 2.39094198e-02]\n",
      "  [9.76385713e-01 3.33710370e-04 2.32805517e-02]\n",
      "  [9.77728546e-01 1.96455527e-04 2.20749602e-02]\n",
      "  [9.81315196e-01 3.03228037e-04 1.83816291e-02]\n",
      "  [9.64648247e-01 6.48855406e-04 3.47029679e-02]\n",
      "  [9.75592554e-01 5.29560435e-04 2.38779485e-02]\n",
      "  [9.72826362e-01 6.20527309e-04 2.65530087e-02]\n",
      "  [9.72455442e-01 9.56017582e-04 2.65884697e-02]\n",
      "  [9.07161951e-01 2.05181427e-02 7.23199397e-02]\n",
      "  [8.86555910e-01 3.31286564e-02 8.03154111e-02]\n",
      "  [8.72796237e-01 4.81281318e-02 7.90756941e-02]\n",
      "  [8.89937818e-01 4.37417552e-02 6.63205013e-02]\n",
      "  [8.77965748e-01 4.95793596e-02 7.24549070e-02]\n",
      "  [8.91572058e-01 3.06820795e-02 7.77459070e-02]\n",
      "  [8.76636446e-01 4.88709249e-02 7.44926408e-02]\n",
      "  [8.99592459e-01 3.79847102e-02 6.24227487e-02]\n",
      "  [8.86673331e-01 4.72574793e-02 6.60691857e-02]\n",
      "  [8.88591051e-01 4.15963382e-02 6.98125884e-02]\n",
      "  [8.71607065e-01 5.52450642e-02 7.31479079e-02]\n",
      "  [8.73108506e-01 6.00763895e-02 6.68151230e-02]\n",
      "  [8.65944326e-01 8.43820050e-02 4.96736728e-02]\n",
      "  [8.57007623e-01 8.47248510e-02 5.82675599e-02]\n",
      "  [8.68153870e-01 8.20408165e-02 4.98053357e-02]\n",
      "  [8.66666496e-01 7.25636929e-02 6.07698262e-02]\n",
      "  [8.53827894e-01 9.73129347e-02 4.88591567e-02]\n",
      "  [8.74127746e-01 6.59121051e-02 5.99600635e-02]\n",
      "  [8.41147900e-01 1.17953368e-01 4.08987962e-02]\n",
      "  [8.69375646e-01 8.44478458e-02 4.61764708e-02]\n",
      "  [8.84335637e-01 7.05957264e-02 4.50685993e-02]\n",
      "  [8.67639899e-01 8.48938823e-02 4.74662669e-02]\n",
      "  [8.54514539e-01 1.00077614e-01 4.54078056e-02]\n",
      "  [8.66823196e-01 8.03018138e-02 5.28749414e-02]\n",
      "  [8.61333489e-01 9.13324505e-02 4.73341011e-02]\n",
      "  [8.85326385e-01 6.52374104e-02 4.94361967e-02]\n",
      "  [8.78302395e-01 6.47896007e-02 5.69080338e-02]\n",
      "  [8.86891127e-01 6.06715493e-02 5.24374396e-02]\n",
      "  [8.55245352e-01 9.45428386e-02 5.02118580e-02]\n",
      "  [8.61901045e-01 6.79039136e-02 7.01950192e-02]\n",
      "  [8.48485410e-01 1.00281551e-01 5.12330756e-02]\n",
      "  [8.61386955e-01 8.18617642e-02 5.67512959e-02]\n",
      "  [8.73608470e-01 6.69656247e-02 5.94258122e-02]\n",
      "  [8.54351580e-01 9.45644006e-02 5.10839894e-02]\n",
      "  [8.64000440e-01 8.18507075e-02 5.41488081e-02]\n",
      "  [8.88500631e-01 5.04159406e-02 6.10835180e-02]\n",
      "  [8.48293245e-01 9.98650864e-02 5.18415980e-02]\n",
      "  [8.80195677e-01 7.43402913e-02 4.54640761e-02]\n",
      "  [8.68223488e-01 7.51105323e-02 5.66659085e-02]\n",
      "  [8.19569230e-01 1.32114515e-01 4.83162999e-02]\n",
      "  [8.70721519e-01 7.45787099e-02 5.46997413e-02]\n",
      "  [8.40559065e-01 1.11875556e-01 4.75654155e-02]\n",
      "  [8.25399458e-01 1.31115586e-01 4.34850194e-02]\n",
      "  [8.40783656e-01 1.05703458e-01 5.35129197e-02]\n",
      "  [8.26229811e-01 1.33679911e-01 4.00901884e-02]\n",
      "  [8.80859196e-01 6.83796108e-02 5.07612377e-02]\n",
      "  [8.00507426e-01 1.52629957e-01 4.68626283e-02]\n",
      "  [8.49271357e-01 1.15236603e-01 3.54920328e-02]\n",
      "  [7.69385219e-01 1.95380926e-01 3.52339074e-02]\n",
      "  [8.43605578e-01 1.02325156e-01 5.40693328e-02]\n",
      "  [8.17323327e-01 1.42869279e-01 3.98074649e-02]\n",
      "  [8.25103581e-01 1.42687112e-01 3.22092623e-02]\n",
      "  [8.92911017e-01 6.57033473e-02 4.13856283e-02]\n",
      "  [8.65864873e-01 8.14878419e-02 5.26473634e-02]\n",
      "  [8.49488914e-01 1.04677573e-01 4.58335057e-02]\n",
      "  [8.94960880e-01 5.81813119e-02 4.68578599e-02]\n",
      "  [8.74187350e-01 8.90201107e-02 3.67926136e-02]\n",
      "  [8.91492188e-01 6.32699355e-02 4.52379510e-02]\n",
      "  [8.80724370e-01 6.40927479e-02 5.51829413e-02]\n",
      "  [9.24111545e-01 2.23717019e-02 5.35166636e-02]\n",
      "  [9.21048343e-01 3.18025611e-02 4.71491478e-02]\n",
      "  [9.29929674e-01 1.90641508e-02 5.10061346e-02]\n",
      "  [9.29109156e-01 1.34808775e-02 5.74099310e-02]\n",
      "  [9.30912077e-01 1.16804289e-02 5.74074797e-02]\n",
      "  [9.20009911e-01 1.37156909e-02 6.62743598e-02]\n",
      "  [9.13971603e-01 2.24591866e-02 6.35691583e-02]\n",
      "  [9.29147124e-01 1.07483165e-02 6.01045489e-02]\n",
      "  [9.09259021e-01 2.00228989e-02 7.07180873e-02]\n",
      "  [9.37117815e-01 7.47441500e-03 5.54077886e-02]\n",
      "  [9.35787439e-01 1.89300869e-02 4.52824384e-02]\n",
      "  [9.36873734e-01 1.38296252e-02 4.92966995e-02]\n",
      "  [9.06666279e-01 3.02600339e-02 6.30736947e-02]\n",
      "  [9.22876716e-01 1.45174274e-02 6.26058951e-02]\n",
      "  [9.32918489e-01 7.75129674e-03 5.93302287e-02]\n",
      "  [9.35550928e-01 6.31465530e-03 5.81344403e-02]\n",
      "  [9.39652562e-01 4.98900283e-03 5.53583875e-02]\n",
      "  [9.38497961e-01 3.62686673e-03 5.78752272e-02]\n",
      "  [9.41490650e-01 4.21073288e-03 5.42986579e-02]\n",
      "  [9.36695755e-01 2.55624996e-03 6.07479066e-02]\n",
      "  [9.48747396e-01 3.55328596e-03 4.76993620e-02]\n",
      "  [9.47565854e-01 3.77816055e-03 4.86560129e-02]\n",
      "  [9.23625946e-01 3.30927526e-03 7.30647668e-02]\n",
      "  [9.25316632e-01 1.93041784e-03 7.27528855e-02]\n",
      "  [9.18576539e-01 2.74148001e-03 7.86820203e-02]\n",
      "  [9.30579066e-01 1.26341055e-03 6.81575835e-02]\n",
      "  [9.45635021e-01 7.64815020e-04 5.36001846e-02]\n",
      "  [9.14075017e-01 1.01482263e-03 8.49102363e-02]\n",
      "  [9.39341605e-01 4.82705946e-05 6.06101714e-02]\n",
      "  [9.48670983e-01 3.21784282e-05 5.12967594e-02]\n",
      "  [9.63212729e-01 1.23015434e-05 3.67749371e-02]\n",
      "  [9.42878902e-01 2.64264178e-04 5.68568371e-02]]], shape=(1, 107, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: saving model to /home/qingyliu/test/training_checkpoint/test\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.7977\n",
      "Epoch 3/500\n",
      "tf.Tensor(\n",
      "[[[5.41273177e-01 7.13760406e-02 3.87350827e-01]\n",
      "  [5.59796870e-01 5.46679795e-02 3.85535151e-01]\n",
      "  [5.80742836e-01 1.15831792e-01 3.03425372e-01]\n",
      "  [6.36162043e-01 9.57356915e-02 2.68102258e-01]\n",
      "  [5.60142934e-01 1.93246827e-01 2.46610269e-01]\n",
      "  [6.27990961e-01 1.58780754e-01 2.13228270e-01]\n",
      "  [6.47249639e-01 1.58642888e-01 1.94107398e-01]\n",
      "  [6.49678469e-01 1.57632917e-01 1.92688704e-01]\n",
      "  [4.86470014e-01 3.84083509e-01 1.29446462e-01]\n",
      "  [4.10048902e-01 5.07690012e-01 8.22610334e-02]\n",
      "  [3.74570012e-01 5.47128737e-01 7.83012435e-02]\n",
      "  [2.93810099e-01 6.44360662e-01 6.18292205e-02]\n",
      "  [4.74223197e-01 4.31872636e-01 9.39041823e-02]\n",
      "  [3.06761712e-01 6.45734847e-01 4.75034043e-02]\n",
      "  [3.28161031e-01 6.24771714e-01 4.70672436e-02]\n",
      "  [2.66091973e-01 7.03531265e-01 3.03767063e-02]\n",
      "  [5.39238751e-02 9.38644528e-01 7.43159419e-03]\n",
      "  [5.09274863e-02 9.42080736e-01 6.99178129e-03]\n",
      "  [3.04422397e-02 9.65665996e-01 3.89179122e-03]\n",
      "  [5.46262674e-02 9.39848959e-01 5.52469445e-03]\n",
      "  [5.08159362e-02 9.44335759e-01 4.84831445e-03]\n",
      "  [3.07614896e-02 9.65900123e-01 3.33841634e-03]\n",
      "  [3.92710604e-02 9.56109583e-01 4.61933576e-03]\n",
      "  [3.16629522e-02 9.64885354e-01 3.45177390e-03]\n",
      "  [3.14968675e-02 9.65658724e-01 2.84433342e-03]\n",
      "  [2.15959512e-02 9.76219475e-01 2.18454329e-03]\n",
      "  [1.94737390e-02 9.78997290e-01 1.52892701e-03]\n",
      "  [2.07581837e-02 9.77476239e-01 1.76560308e-03]\n",
      "  [1.35574406e-02 9.85265315e-01 1.17724051e-03]\n",
      "  [1.67756453e-02 9.81769085e-01 1.45534042e-03]\n",
      "  [9.95372981e-03 9.89228606e-01 8.17669672e-04]\n",
      "  [1.24596870e-02 9.86376822e-01 1.16351445e-03]\n",
      "  [9.76326130e-03 9.89532948e-01 7.03928177e-04]\n",
      "  [1.41747333e-02 9.84393179e-01 1.43216061e-03]\n",
      "  [1.07403267e-02 9.88466859e-01 7.92820472e-04]\n",
      "  [1.62342135e-02 9.82102513e-01 1.66326878e-03]\n",
      "  [1.67356022e-02 9.81951237e-01 1.31315144e-03]\n",
      "  [1.34025542e-02 9.85367835e-01 1.22963940e-03]\n",
      "  [1.33685013e-02 9.85425591e-01 1.20581640e-03]\n",
      "  [1.67185403e-02 9.81834590e-01 1.44692615e-03]\n",
      "  [8.73522274e-03 9.90473449e-01 7.91288563e-04]\n",
      "  [1.71484817e-02 9.81229663e-01 1.62184157e-03]\n",
      "  [9.37897898e-03 9.89919782e-01 7.01173733e-04]\n",
      "  [1.35526657e-02 9.85008001e-01 1.43936044e-03]\n",
      "  [7.05238618e-03 9.92343128e-01 6.04504778e-04]\n",
      "  [1.42011410e-02 9.84546423e-01 1.25242001e-03]\n",
      "  [1.13853738e-02 9.87738132e-01 8.76563718e-04]\n",
      "  [1.36975711e-02 9.84938383e-01 1.36400375e-03]\n",
      "  [1.06550790e-02 9.88565862e-01 7.79127062e-04]\n",
      "  [1.45653067e-02 9.84085083e-01 1.34960061e-03]\n",
      "  [7.05555687e-03 9.92353499e-01 5.91063057e-04]\n",
      "  [1.67320073e-02 9.81856048e-01 1.41187431e-03]\n",
      "  [1.02315694e-02 9.89023328e-01 7.45091529e-04]\n",
      "  [7.41446577e-03 9.91915405e-01 6.70041423e-04]\n",
      "  [9.94824711e-03 9.89285409e-01 7.66264740e-04]\n",
      "  [1.01827467e-02 9.88819540e-01 9.97750671e-04]\n",
      "  [7.90124666e-03 9.91440177e-01 6.58597390e-04]\n",
      "  [9.93319042e-03 9.89106476e-01 9.60335135e-04]\n",
      "  [5.40189631e-03 9.94198203e-01 3.99900076e-04]\n",
      "  [1.13570653e-02 9.87807453e-01 8.35522893e-04]\n",
      "  [6.24581845e-03 9.93327379e-01 4.26807936e-04]\n",
      "  [5.50406985e-03 9.94120896e-01 3.74991971e-04]\n",
      "  [6.79018954e-03 9.92759585e-01 4.50230349e-04]\n",
      "  [5.50679397e-03 9.94096637e-01 3.96553631e-04]\n",
      "  [7.74794212e-03 9.91737068e-01 5.14958927e-04]\n",
      "  [6.06380543e-03 9.93589997e-01 3.46186629e-04]\n",
      "  [4.67015011e-03 9.95017827e-01 3.12067627e-04]\n",
      "  [8.81078560e-03 9.90565717e-01 6.23450091e-04]\n",
      "  [6.92591956e-03 9.92569149e-01 5.04829746e-04]\n",
      "  [5.87764103e-03 9.93694007e-01 4.28356027e-04]\n",
      "  [1.14295296e-02 9.87714767e-01 8.55744409e-04]\n",
      "  [8.18196405e-03 9.91159379e-01 6.58762525e-04]\n",
      "  [1.31642101e-02 9.85842407e-01 9.93359368e-04]\n",
      "  [1.66763440e-02 9.82125044e-01 1.19868817e-03]\n",
      "  [1.36902919e-02 9.85219836e-01 1.08994928e-03]\n",
      "  [2.93399133e-02 9.67747092e-01 2.91294232e-03]\n",
      "  [2.13958547e-02 9.76590216e-01 2.01387913e-03]\n",
      "  [2.01191567e-02 9.77767527e-01 2.11336580e-03]\n",
      "  [4.40017246e-02 9.50622439e-01 5.37579507e-03]\n",
      "  [1.72824766e-02 9.81094480e-01 1.62304984e-03]\n",
      "  [2.79896110e-02 9.68860447e-01 3.14996182e-03]\n",
      "  [1.91407949e-02 9.79020178e-01 1.83906197e-03]\n",
      "  [3.29043418e-02 9.63804662e-01 3.29097384e-03]\n",
      "  [4.56809737e-02 9.50680315e-01 3.63872107e-03]\n",
      "  [2.63576880e-02 9.70282137e-01 3.36024282e-03]\n",
      "  [3.80615108e-02 9.57978845e-01 3.95968650e-03]\n",
      "  [4.86760065e-02 9.46286857e-01 5.03718108e-03]\n",
      "  [2.90472955e-02 9.67627525e-01 3.32523021e-03]\n",
      "  [4.16771024e-02 9.52470541e-01 5.85235748e-03]\n",
      "  [5.17997555e-02 9.39670622e-01 8.52954853e-03]\n",
      "  [5.26048541e-02 9.38529313e-01 8.86581000e-03]\n",
      "  [1.52798653e-01 8.20246041e-01 2.69553084e-02]\n",
      "  [1.11321323e-01 8.65832806e-01 2.28458568e-02]\n",
      "  [1.20514221e-01 8.57350051e-01 2.21356899e-02]\n",
      "  [1.74939498e-01 7.80364931e-01 4.46955562e-02]\n",
      "  [1.19182982e-01 8.55675399e-01 2.51415260e-02]\n",
      "  [7.56430924e-02 9.09245193e-01 1.51116783e-02]\n",
      "  [1.44250572e-01 8.22581351e-01 3.31680514e-02]\n",
      "  [1.33273214e-01 8.34622562e-01 3.21041942e-02]\n",
      "  [1.62986442e-01 7.77732730e-01 5.92808612e-02]\n",
      "  [1.94813177e-01 7.38677442e-01 6.65093809e-02]\n",
      "  [2.57251412e-01 6.36238694e-01 1.06509954e-01]\n",
      "  [3.11922342e-01 5.48442662e-01 1.39634982e-01]\n",
      "  [4.47946578e-01 3.06158900e-01 2.45894492e-01]\n",
      "  [4.70484883e-01 2.47805819e-01 2.81709254e-01]\n",
      "  [4.77776945e-01 1.43353611e-01 3.78869414e-01]\n",
      "  [4.19493496e-01 2.02996105e-01 3.77510428e-01]]], shape=(1, 107, 3), dtype=float32)\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0237\n",
      "Epoch 4/500\n",
      "tf.Tensor(\n",
      "[[[5.72164893e-01 8.34772289e-02 3.44357848e-01]\n",
      "  [5.84648490e-01 4.35098112e-02 3.71841669e-01]\n",
      "  [6.82637453e-01 3.31905782e-02 2.84171999e-01]\n",
      "  [7.22665548e-01 1.64097510e-02 2.60924697e-01]\n",
      "  [7.55118310e-01 7.27120182e-03 2.37610474e-01]\n",
      "  [7.65580058e-01 5.99754183e-03 2.28422418e-01]\n",
      "  [7.95912385e-01 4.35316609e-03 1.99734390e-01]\n",
      "  [8.17614496e-01 2.03210604e-03 1.80353358e-01]\n",
      "  [8.38462532e-01 3.62915616e-03 1.57908291e-01]\n",
      "  [8.40540171e-01 3.28563107e-03 1.56174168e-01]\n",
      "  [8.48534763e-01 3.52657028e-03 1.47938579e-01]\n",
      "  [8.55723619e-01 1.63978012e-03 1.42636538e-01]\n",
      "  [8.73304248e-01 2.10928661e-03 1.24586403e-01]\n",
      "  [8.90189946e-01 2.47244188e-03 1.07337572e-01]\n",
      "  [8.87938797e-01 1.11008273e-03 1.10951148e-01]\n",
      "  [8.97550344e-01 6.51050068e-04 1.01798594e-01]\n",
      "  [8.86168718e-01 3.48807080e-03 1.10343128e-01]\n",
      "  [8.57042372e-01 2.44016759e-03 1.40517473e-01]\n",
      "  [8.93657565e-01 2.40405160e-03 1.03938423e-01]\n",
      "  [8.88648868e-01 1.60254759e-03 1.09748580e-01]\n",
      "  [9.29886639e-01 1.27619237e-03 6.88371509e-02]\n",
      "  [9.05729234e-01 1.10039487e-03 9.31702927e-02]\n",
      "  [8.95315945e-01 1.08518160e-03 1.03598841e-01]\n",
      "  [9.17240083e-01 1.11282861e-03 8.16471726e-02]\n",
      "  [9.00818944e-01 4.52258973e-04 9.87287611e-02]\n",
      "  [9.18774545e-01 8.72960081e-04 8.03525746e-02]\n",
      "  [9.14672613e-01 1.19390420e-03 8.41335058e-02]\n",
      "  [9.24637198e-01 6.70179958e-04 7.46926069e-02]\n",
      "  [9.09008145e-01 4.77534195e-04 9.05143246e-02]\n",
      "  [9.35487986e-01 7.56035559e-04 6.37560412e-02]\n",
      "  [9.23430800e-01 1.07426802e-03 7.54949078e-02]\n",
      "  [9.07828927e-01 9.64850595e-04 9.12061930e-02]\n",
      "  [9.13839519e-01 8.09052261e-04 8.53513926e-02]\n",
      "  [9.15130734e-01 1.31948839e-03 8.35497603e-02]\n",
      "  [9.12139058e-01 1.11296948e-03 8.67479369e-02]\n",
      "  [9.06990469e-01 1.35259598e-03 9.16569680e-02]\n",
      "  [8.94783616e-01 1.21026277e-03 1.04006186e-01]\n",
      "  [8.87281418e-01 1.40638882e-03 1.11312255e-01]\n",
      "  [9.03974414e-01 1.22164504e-03 9.48039442e-02]\n",
      "  [8.75373065e-01 1.89375714e-03 1.22733198e-01]\n",
      "  [9.00844216e-01 1.02191709e-03 9.81339142e-02]\n",
      "  [8.81327033e-01 2.05704500e-03 1.16615973e-01]\n",
      "  [8.80378008e-01 1.69077225e-03 1.17931299e-01]\n",
      "  [8.87288094e-01 2.34627794e-03 1.10365666e-01]\n",
      "  [8.83575439e-01 2.16608075e-03 1.14258379e-01]\n",
      "  [8.84782255e-01 1.88469770e-03 1.13333046e-01]\n",
      "  [8.92039537e-01 2.03545345e-03 1.05925046e-01]\n",
      "  [8.76788199e-01 1.73304719e-03 1.21478714e-01]\n",
      "  [8.82250845e-01 2.06900924e-03 1.15680084e-01]\n",
      "  [8.96218777e-01 2.60491879e-03 1.01176284e-01]\n",
      "  [9.12189305e-01 1.57819199e-03 8.62324610e-02]\n",
      "  [9.04588938e-01 1.62641704e-03 9.37846452e-02]\n",
      "  [8.87025595e-01 1.54328882e-03 1.11431122e-01]\n",
      "  [8.98171365e-01 1.55396166e-03 1.00274727e-01]\n",
      "  [8.94266784e-01 1.26194768e-03 1.04471192e-01]\n",
      "  [9.08402860e-01 1.32355222e-03 9.02736261e-02]\n",
      "  [8.87887299e-01 1.03161379e-03 1.11081101e-01]\n",
      "  [8.92264128e-01 1.63817639e-03 1.06097668e-01]\n",
      "  [9.13630307e-01 9.43464285e-04 8.54262188e-02]\n",
      "  [9.01054680e-01 8.04376614e-04 9.81409773e-02]\n",
      "  [8.99095535e-01 1.34165422e-03 9.95627567e-02]\n",
      "  [9.02231157e-01 8.11061822e-04 9.69577357e-02]\n",
      "  [9.12279189e-01 5.73018857e-04 8.71477872e-02]\n",
      "  [9.29330468e-01 4.66430240e-04 7.02031478e-02]\n",
      "  [9.20808494e-01 5.45441057e-04 7.86460936e-02]\n",
      "  [9.10150349e-01 6.73453731e-04 8.91762376e-02]\n",
      "  [9.25499260e-01 5.44514623e-04 7.39562437e-02]\n",
      "  [9.15041983e-01 3.49907408e-04 8.46081004e-02]\n",
      "  [9.12683070e-01 5.38595719e-04 8.67783874e-02]\n",
      "  [9.05678034e-01 7.10361055e-04 9.36115310e-02]\n",
      "  [9.02103603e-01 4.09066270e-04 9.74873677e-02]\n",
      "  [9.18101788e-01 6.51316019e-04 8.12468976e-02]\n",
      "  [8.80915463e-01 7.23889389e-04 1.18360706e-01]\n",
      "  [9.12318468e-01 4.09368629e-04 8.72721374e-02]\n",
      "  [9.19584930e-01 7.04361475e-04 7.97107220e-02]\n",
      "  [8.94381881e-01 5.08110912e-04 1.05109923e-01]\n",
      "  [8.89958084e-01 4.71555453e-04 1.09570295e-01]\n",
      "  [8.93940806e-01 5.24067960e-04 1.05535097e-01]\n",
      "  [9.16267931e-01 4.52876178e-04 8.32791701e-02]\n",
      "  [9.05918896e-01 5.33816405e-04 9.35472623e-02]\n",
      "  [8.91848385e-01 4.65819205e-04 1.07685864e-01]\n",
      "  [8.75524342e-01 1.46978954e-03 1.23005867e-01]\n",
      "  [8.90862286e-01 7.97312299e-04 1.08340330e-01]\n",
      "  [9.01330173e-01 8.00400623e-04 9.78694484e-02]\n",
      "  [8.92553985e-01 4.50401712e-04 1.06995627e-01]\n",
      "  [8.90152991e-01 9.75900213e-04 1.08870998e-01]\n",
      "  [8.65492463e-01 1.15262379e-03 1.33354887e-01]\n",
      "  [8.86707067e-01 1.19612890e-03 1.12096801e-01]\n",
      "  [8.72779429e-01 1.37479242e-03 1.25845671e-01]\n",
      "  [8.56665909e-01 1.45171210e-03 1.41882434e-01]\n",
      "  [8.24535966e-01 3.12652462e-03 1.72337517e-01]\n",
      "  [8.59611392e-01 4.46128054e-03 1.35927290e-01]\n",
      "  [8.11914980e-01 6.44153683e-03 1.81643486e-01]\n",
      "  [8.36236358e-01 8.17330927e-03 1.55590296e-01]\n",
      "  [8.18900526e-01 8.71853437e-03 1.72380969e-01]\n",
      "  [7.92482436e-01 2.25238036e-02 1.84993759e-01]\n",
      "  [7.71318316e-01 3.22555266e-02 1.96426138e-01]\n",
      "  [7.33498573e-01 2.37170774e-02 2.42784292e-01]\n",
      "  [7.60436177e-01 2.48391144e-02 2.14724705e-01]\n",
      "  [6.59737051e-01 5.13671152e-02 2.88895816e-01]\n",
      "  [6.29131675e-01 5.93651161e-02 3.11503172e-01]\n",
      "  [6.09941185e-01 8.43166262e-02 3.05742174e-01]\n",
      "  [5.58536530e-01 1.19545631e-01 3.21917832e-01]\n",
      "  [5.62295377e-01 1.04551695e-01 3.33152980e-01]\n",
      "  [5.35340250e-01 1.05111673e-01 3.59548062e-01]\n",
      "  [4.74065512e-01 1.24956682e-01 4.00977731e-01]\n",
      "  [4.30174053e-01 1.87079221e-01 3.82746726e-01]]], shape=(1, 107, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: saving model to /home/qingyliu/test/training_checkpoint/test\n",
      "1/1 [==============================] - 8s 8s/step - loss: 3.2852\n",
      "Epoch 5/500\n",
      "tf.Tensor(\n",
      "[[[0.43948415 0.2638154  0.29670048]\n",
      "  [0.4366835  0.28238386 0.28093266]\n",
      "  [0.43468124 0.30702204 0.25829673]\n",
      "  [0.4759361  0.20806892 0.315995  ]\n",
      "  [0.49751374 0.13047506 0.37201115]\n",
      "  [0.5188056  0.15326113 0.32793322]\n",
      "  [0.49782938 0.1644263  0.33774427]\n",
      "  [0.53070855 0.10207319 0.36721826]\n",
      "  [0.45503014 0.32073298 0.22423688]\n",
      "  [0.49614546 0.24373664 0.2601179 ]\n",
      "  [0.49832153 0.24548753 0.25619093]\n",
      "  [0.4746618  0.25653845 0.26879972]\n",
      "  [0.5323099  0.23260978 0.23508027]\n",
      "  [0.49300244 0.23721568 0.2697818 ]\n",
      "  [0.52898633 0.21425955 0.2567541 ]\n",
      "  [0.5347276  0.20043394 0.26483852]\n",
      "  [0.3749112  0.4445157  0.18057312]\n",
      "  [0.41946182 0.38279915 0.19773905]\n",
      "  [0.39171022 0.4152949  0.19299485]\n",
      "  [0.40886077 0.39159593 0.19954328]\n",
      "  [0.44702604 0.33510202 0.21787192]\n",
      "  [0.4396817  0.32004914 0.24026915]\n",
      "  [0.46284416 0.32318458 0.21397129]\n",
      "  [0.41981667 0.3612225  0.21896082]\n",
      "  [0.4803353  0.26309416 0.25657055]\n",
      "  [0.46343088 0.3125143  0.22405481]\n",
      "  [0.433872   0.3358421  0.23028588]\n",
      "  [0.48122552 0.2616165  0.25715795]\n",
      "  [0.48947513 0.22231248 0.2882124 ]\n",
      "  [0.46610862 0.2811494  0.25274202]\n",
      "  [0.47188276 0.27619424 0.251923  ]\n",
      "  [0.4576742  0.30647114 0.2358546 ]\n",
      "  [0.43074474 0.33966255 0.22959271]\n",
      "  [0.4804205  0.24400021 0.2755793 ]\n",
      "  [0.46715683 0.2936301  0.23921302]\n",
      "  [0.48980585 0.2244487  0.28574547]\n",
      "  [0.47111702 0.24355532 0.2853276 ]\n",
      "  [0.48382148 0.23035096 0.28582755]\n",
      "  [0.43660387 0.31566593 0.24773023]\n",
      "  [0.42795277 0.31876135 0.25328586]\n",
      "  [0.45399606 0.27386114 0.2721428 ]\n",
      "  [0.45128813 0.31074086 0.23797101]\n",
      "  [0.45198873 0.25395092 0.29406035]\n",
      "  [0.44496852 0.28769597 0.26733547]\n",
      "  [0.4475397  0.26132065 0.29113963]\n",
      "  [0.45185876 0.25576895 0.2923723 ]\n",
      "  [0.47083145 0.2300054  0.2991632 ]\n",
      "  [0.45568517 0.25980327 0.28451154]\n",
      "  [0.4531436  0.26824698 0.27860942]\n",
      "  [0.49006528 0.20053837 0.3093963 ]\n",
      "  [0.4703247  0.2016651  0.32801017]\n",
      "  [0.45895204 0.2800309  0.2610171 ]\n",
      "  [0.4201061  0.320714   0.25917992]\n",
      "  [0.47929803 0.20523213 0.31546986]\n",
      "  [0.49075565 0.20565608 0.3035883 ]\n",
      "  [0.48642367 0.21756446 0.29601178]\n",
      "  [0.47379726 0.2431268  0.28307596]\n",
      "  [0.47191164 0.24612701 0.28196138]\n",
      "  [0.4755774  0.23938265 0.28503993]\n",
      "  [0.459786   0.2516432  0.28857076]\n",
      "  [0.5075647  0.18156755 0.3108677 ]\n",
      "  [0.49022335 0.1967892  0.31298745]\n",
      "  [0.49232277 0.18835571 0.31932157]\n",
      "  [0.4928444  0.16777113 0.33938444]\n",
      "  [0.48024943 0.19725123 0.32249936]\n",
      "  [0.5088145  0.15886188 0.33232358]\n",
      "  [0.5191264  0.16017416 0.3206994 ]\n",
      "  [0.49453002 0.16635454 0.33911538]\n",
      "  [0.47602847 0.22548187 0.29848963]\n",
      "  [0.50637555 0.18687624 0.30674824]\n",
      "  [0.47557324 0.21448709 0.3099396 ]\n",
      "  [0.51924574 0.17084591 0.3099084 ]\n",
      "  [0.49272445 0.17944753 0.32782796]\n",
      "  [0.5217966  0.14792025 0.3302832 ]\n",
      "  [0.48280537 0.22395426 0.2932404 ]\n",
      "  [0.49239877 0.16841947 0.3391818 ]\n",
      "  [0.48565707 0.19168925 0.3226537 ]\n",
      "  [0.48813722 0.19095695 0.3209058 ]\n",
      "  [0.48303488 0.21456173 0.3024034 ]\n",
      "  [0.5137061  0.16274145 0.32355252]\n",
      "  [0.5008012  0.18951996 0.3096789 ]\n",
      "  [0.47159174 0.24507876 0.2833295 ]\n",
      "  [0.4904951  0.23002288 0.27948204]\n",
      "  [0.4654889  0.24372058 0.29079053]\n",
      "  [0.48703122 0.2230203  0.2899485 ]\n",
      "  [0.4664441  0.26059976 0.27295613]\n",
      "  [0.48326954 0.22584195 0.29088858]\n",
      "  [0.47336152 0.2893542  0.2372843 ]\n",
      "  [0.45405737 0.30137783 0.24456486]\n",
      "  [0.4238768  0.36511412 0.21100904]\n",
      "  [0.44429633 0.32266393 0.23303974]\n",
      "  [0.3818054  0.4379735  0.18022116]\n",
      "  [0.36292335 0.46947512 0.16760156]\n",
      "  [0.26883343 0.6138515  0.11731512]\n",
      "  [0.28543058 0.5969158  0.11765362]\n",
      "  [0.27589163 0.6134801  0.1106282 ]\n",
      "  [0.28839898 0.5909726  0.12062841]\n",
      "  [0.2543151  0.6401168  0.10556813]\n",
      "  [0.20041491 0.71810466 0.08148041]\n",
      "  [0.2184885  0.6844334  0.09707814]\n",
      "  [0.19210093 0.72411734 0.0837818 ]\n",
      "  [0.18087435 0.7403912  0.07873451]\n",
      "  [0.16836463 0.75924444 0.07239091]\n",
      "  [0.21158245 0.693532   0.09488557]\n",
      "  [0.20528594 0.7013311  0.093383  ]\n",
      "  [0.25342774 0.6032468  0.14332552]\n",
      "  [0.26554745 0.5750464  0.15940614]]], shape=(1, 107, 3), dtype=float32)\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.2902\n",
      "Epoch 6/500\n",
      "tf.Tensor(\n",
      "[[[0.3880549  0.33451375 0.27743134]\n",
      "  [0.34497538 0.42876127 0.22626333]\n",
      "  [0.31295586 0.4906568  0.19638729]\n",
      "  [0.35471794 0.40774882 0.23753324]\n",
      "  [0.3962776  0.32211548 0.28160688]\n",
      "  [0.3732478  0.37474775 0.2520044 ]\n",
      "  [0.41586187 0.2902635  0.29387456]\n",
      "  [0.39021674 0.3389987  0.27078453]\n",
      "  [0.19619128 0.71333134 0.09047745]\n",
      "  [0.19101028 0.7221847  0.08680497]\n",
      "  [0.2578447  0.6145021  0.12765327]\n",
      "  [0.2398473  0.63572323 0.12442943]\n",
      "  [0.26622748 0.59791505 0.13585745]\n",
      "  [0.25716135 0.61794597 0.12489263]\n",
      "  [0.25633955 0.6121643  0.13149609]\n",
      "  [0.26474044 0.60424644 0.13101305]\n",
      "  [0.12765303 0.8222893  0.05005769]\n",
      "  [0.10317514 0.85667145 0.04015344]\n",
      "  [0.12259883 0.82868177 0.04871939]\n",
      "  [0.12937567 0.8172566  0.0533677 ]\n",
      "  [0.14801665 0.7879214  0.0640619 ]\n",
      "  [0.15957625 0.7683791  0.07204471]\n",
      "  [0.15778813 0.77349496 0.0687168 ]\n",
      "  [0.16802382 0.75538135 0.07659484]\n",
      "  [0.16938221 0.7502052  0.08041254]\n",
      "  [0.1408004  0.7954422  0.06375731]\n",
      "  [0.17743883 0.7386258  0.08393541]\n",
      "  [0.1798705  0.7339687  0.08616082]\n",
      "  [0.17217106 0.7473925  0.08043655]\n",
      "  [0.19077595 0.7172208  0.09200326]\n",
      "  [0.14494903 0.78781927 0.06723174]\n",
      "  [0.16417852 0.7587964  0.07702513]\n",
      "  [0.19690205 0.71018946 0.09290847]\n",
      "  [0.18596353 0.72259957 0.09143687]\n",
      "  [0.21053599 0.68159264 0.10787135]\n",
      "  [0.23126034 0.6430523  0.12568733]\n",
      "  [0.25094852 0.60704535 0.14200614]\n",
      "  [0.23210306 0.6435045  0.12439241]\n",
      "  [0.2301705  0.64655894 0.12327062]\n",
      "  [0.21092826 0.67602557 0.11304621]\n",
      "  [0.2352185  0.63943213 0.12534936]\n",
      "  [0.2109217  0.6741322  0.11494608]\n",
      "  [0.2327076  0.64315563 0.12413679]\n",
      "  [0.21728769 0.6630696  0.11964268]\n",
      "  [0.21722977 0.6644192  0.11835103]\n",
      "  [0.22879447 0.6460317  0.12517385]\n",
      "  [0.2292364  0.6401259  0.13063781]\n",
      "  [0.22772805 0.6487775  0.12349442]\n",
      "  [0.22845526 0.64629847 0.12524627]\n",
      "  [0.21266936 0.67366195 0.11366875]\n",
      "  [0.22928955 0.64276206 0.12794833]\n",
      "  [0.1978686  0.69567204 0.10645935]\n",
      "  [0.19852757 0.69680226 0.10467016]\n",
      "  [0.20311674 0.68815976 0.10872339]\n",
      "  [0.2646592  0.5790716  0.15626925]\n",
      "  [0.23232523 0.64091295 0.12676182]\n",
      "  [0.20507608 0.68636256 0.10856131]\n",
      "  [0.2231419  0.65385276 0.12300533]\n",
      "  [0.22198154 0.6570665  0.12095191]\n",
      "  [0.21387993 0.6748183  0.11130183]\n",
      "  [0.24678688 0.6131024  0.14011079]\n",
      "  [0.26485848 0.590174   0.14496747]\n",
      "  [0.2548442  0.6017091  0.14344665]\n",
      "  [0.25118205 0.6045207  0.14429726]\n",
      "  [0.25919288 0.5943572  0.14644988]\n",
      "  [0.25138164 0.60656583 0.14205258]\n",
      "  [0.22489499 0.64896804 0.12613693]\n",
      "  [0.24842434 0.6096951  0.14188062]\n",
      "  [0.24082889 0.6277155  0.13145554]\n",
      "  [0.2307538  0.6400486  0.1291976 ]\n",
      "  [0.2607942  0.58810955 0.1510963 ]\n",
      "  [0.26642367 0.58301175 0.15056461]\n",
      "  [0.23065777 0.6443282  0.12501407]\n",
      "  [0.2560523  0.5944909  0.14945677]\n",
      "  [0.21409057 0.6756613  0.11024803]\n",
      "  [0.26877084 0.5749113  0.15631786]\n",
      "  [0.27118915 0.57224536 0.15656546]\n",
      "  [0.28398338 0.55862004 0.15739661]\n",
      "  [0.26442426 0.58442765 0.151148  ]\n",
      "  [0.2609725  0.59608924 0.14293824]\n",
      "  [0.26905176 0.57555    0.15539816]\n",
      "  [0.24570318 0.6225209  0.13177587]\n",
      "  [0.20404957 0.69299465 0.10295578]\n",
      "  [0.23425019 0.6470057  0.11874419]\n",
      "  [0.24728183 0.6252675  0.12745067]\n",
      "  [0.2001171  0.7030422  0.09684075]\n",
      "  [0.1969461  0.70945334 0.09360054]\n",
      "  [0.21136962 0.6847597  0.10387069]\n",
      "  [0.1937249  0.71439743 0.09187766]\n",
      "  [0.1699961  0.75690645 0.07309748]\n",
      "  [0.19464242 0.71514887 0.09020872]\n",
      "  [0.18446626 0.7336139  0.08191992]\n",
      "  [0.16248369 0.7666458  0.07087057]\n",
      "  [0.14326383 0.7991762  0.05755992]\n",
      "  [0.12822106 0.82334197 0.04843704]\n",
      "  [0.10076376 0.8617113  0.03752482]\n",
      "  [0.10540682 0.8561076  0.03848563]\n",
      "  [0.11541265 0.84109646 0.04349094]\n",
      "  [0.1008064  0.86274743 0.03644622]\n",
      "  [0.13097586 0.81619847 0.05282568]\n",
      "  [0.11609598 0.8389498  0.04495414]\n",
      "  [0.10175154 0.8607869  0.0374615 ]\n",
      "  [0.09337462 0.872328   0.03429735]\n",
      "  [0.15490821 0.7804348  0.06465694]\n",
      "  [0.15611336 0.77703774 0.06684891]\n",
      "  [0.17195411 0.7423258  0.08572016]\n",
      "  [0.19126038 0.705729   0.10301063]]], shape=(1, 107, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(input_generator(\"/nfs/amino-home/qingyliu/dihedral_angle/temp\", \"/nfs/amino-home/qingyliu/dihedral_angle/chi1_label\",epochs = 500),\n",
    "                             steps_per_epoch = 1,\n",
    "                             epochs = 500,\n",
    "                             callbacks = [cp_callback_all_conv])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
