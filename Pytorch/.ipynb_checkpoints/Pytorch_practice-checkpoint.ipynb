{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialDataset(Dataset):\n",
    "    #Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, feature_path, label_path):\n",
    "        'Initialization'\n",
    "        self.features = np.load(feature_path)\n",
    "        self.labels = np.load(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return np.shape(self.labels)[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        #ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        #X = torch.load('data/' + ID + '.pt')\n",
    "        \n",
    "        #y = self.labels[ID]\n",
    "\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrialDataset(\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/back_train_feature_new_0.npy\",\"/nfs/amino-home/qingyliu/dihedral_angle/ML_data/back_tag/back_train_label_new_0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 2,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = torch.nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            torch.nn.Conv1d(27,16 , kernel_size=3, stride=2),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # Defining another 2D convolution layer\n",
    "            torch.nn.Conv1d(16, 8, kernel_size=3, stride=2),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # Defining another 2D convolution layer\n",
    "            torch.nn.Conv1d(8, 3, kernel_size=3),\n",
    "            torch.nn.BatchNorm1d(3),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.linear_layers = torch.nn.Sequential(\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv1d(27, 16, kernel_size=(3,), stride=(2,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(16, 8, kernel_size=(3,), stride=(2,))\n",
      "    (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv1d(8, 3, kernel_size=(3,), stride=(1,))\n",
      "    (7): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(val_x), Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f00b401f150>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.2350522 , 0.02231344, 0.80533842, 0.98579704, 0.02847059,\n",
       "         0.09449037, 0.4061269 , 0.0319844 , 0.88894403, 0.04787969,\n",
       "         0.13124447, 0.41580948, 0.18242552, 0.9393461 , 0.84553473,\n",
       "         0.39651675, 0.25161828, 0.05896701, 0.04228977, 0.09708864,\n",
       "         0.999     , 0.001     , 0.        , 0.09701467, 0.33574106,\n",
       "         0.56724427, 0.        ],\n",
       "        [0.65475346, 0.9793667 , 0.21585281, 0.28090034, 0.06180347,\n",
       "         0.21081829, 0.97234758, 0.06238585, 0.26114999, 0.06122616,\n",
       "         0.12238889, 0.4061269 , 0.9393461 , 0.31647911, 0.18542742,\n",
       "         0.95689275, 0.53245431, 0.11105597, 0.03992533, 0.21081829,\n",
       "         0.856     , 0.123     , 0.01      , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        ],\n",
       "        [0.6637387 , 0.82635335, 0.10340045, 0.30576366, 0.03992533,\n",
       "         0.09708864, 0.68997448, 0.04436183, 0.75212911, 0.07242649,\n",
       "         0.13705129, 0.24048908, 0.74838172, 0.53245431, 0.99517928,\n",
       "         0.31002552, 0.2042403 , 0.08393843, 0.03841999, 0.11304583,\n",
       "         0.714     , 0.292     , 0.029     , 0.09659795, 0.34668396,\n",
       "         0.55671809, 0.        ],\n",
       "        [0.07043673, 0.69635493, 0.01729316, 0.03879113, 0.99440349,\n",
       "         0.02083634, 0.56463629, 0.17799369, 0.04697615, 0.24973989,\n",
       "         0.21925725, 0.04069905, 0.01889096, 0.06180347, 0.05520054,\n",
       "         0.0684976 , 0.07798824, 0.14931293, 0.75951092, 0.99885362,\n",
       "         0.69      , 0.34      , 0.019     , 0.11532186, 0.35473446,\n",
       "         0.52994368, 0.        ],\n",
       "        [0.68352089, 0.47751518, 0.70889017, 0.32959884, 0.04787969,\n",
       "         0.93877384, 0.36586441, 0.07516011, 0.55477924, 0.05896701,\n",
       "         0.11920292, 0.4378235 , 0.14554233, 0.34524654, 0.76314502,\n",
       "         0.9020312 , 0.43536371, 0.19154535, 0.04031042, 0.08627419,\n",
       "         0.762     , 0.243     , 0.01      , 0.45021586, 0.25275058,\n",
       "         0.29703357, 0.        ],\n",
       "        [0.84157582, 0.19466158, 0.20261985, 0.23685498, 0.08091347,\n",
       "         0.77902611, 0.1358729 , 0.15187116, 0.25161828, 0.37519353,\n",
       "         0.22970105, 0.36818758, 0.16798161, 0.26114999, 0.159762  ,\n",
       "         0.96770454, 0.82200631, 0.2592251 , 0.04436183, 0.09708864,\n",
       "         0.621     , 0.331     , 0.018     , 0.09014089, 0.7253045 ,\n",
       "         0.18455461, 0.        ],\n",
       "        [0.11920292, 0.08866866, 0.01406363, 0.03916572, 0.32519473,\n",
       "         0.0190772 , 0.05678618, 0.61063923, 0.0710943 , 0.9488263 ,\n",
       "         0.9995097 , 0.03076886, 0.02659699, 0.15446527, 0.07446795,\n",
       "         0.06416388, 0.15187116, 0.4725277 , 0.08091347, 0.12564786,\n",
       "         0.83      , 0.153     , 0.023     , 0.08247777, 0.30317817,\n",
       "         0.61434406, 0.        ],\n",
       "        [0.47502081, 0.04697615, 0.63645254, 0.78583498, 0.0319844 ,\n",
       "         0.22793645, 0.46008512, 0.04879972, 0.7349726 , 0.08627419,\n",
       "         0.14931293, 0.45512111, 0.9793667 , 0.94267582, 0.83616964,\n",
       "         0.5149955 , 0.32739298, 0.07943855, 0.03732689, 0.08948006,\n",
       "         0.833     , 0.149     , 0.015     , 0.46863296, 0.09648513,\n",
       "         0.43488191, 0.        ],\n",
       "        [0.18093879, 0.01964677, 0.22618143, 0.75026011, 0.02003571,\n",
       "         0.08241332, 0.2441611 , 0.02792257, 0.99456799, 0.03992533,\n",
       "         0.12238889, 0.30153478, 0.13354172, 0.92956327, 0.93213771,\n",
       "         0.2890505 , 0.19623406, 0.04436183, 0.02711972, 0.07516011,\n",
       "         0.06      , 0.949     , 0.001     , 0.08121072, 0.34396771,\n",
       "         0.57482158, 0.        ],\n",
       "        [0.7047457 , 0.74459692, 0.37285223, 0.47003595, 0.05952437,\n",
       "         0.73105858, 0.13705129, 0.15187116, 0.26114999, 0.15577584,\n",
       "         0.17079548, 0.35893259, 0.86176173, 0.28495789, 0.15446527,\n",
       "         0.9488263 , 0.77902611, 0.37989357, 0.03805225, 0.08548914,\n",
       "         0.088     , 0.929     , 0.        , 0.45021586, 0.25275058,\n",
       "         0.29703357, 0.        ],\n",
       "        [0.20915937, 0.01378899, 0.97314288, 0.99411824, 0.01646364,\n",
       "         0.07043673, 0.27888482, 0.01926523, 0.47502081, 0.02343067,\n",
       "         0.05952437, 0.38698582, 0.12675058, 0.90550963, 0.27687819,\n",
       "         0.31002552, 0.17079548, 0.03522972, 0.02124832, 0.04973651,\n",
       "         0.066     , 0.952     , 0.        , 0.09701467, 0.33574106,\n",
       "         0.56724427, 0.        ],\n",
       "        [0.09363821, 0.11105597, 0.01056069, 0.02320294, 0.43045378,\n",
       "         0.01025177, 0.02460243, 0.70889017, 0.03522972, 0.99405948,\n",
       "         0.80059224, 0.01325548, 0.02275394, 0.04833763, 0.04697615,\n",
       "         0.03522972, 0.12025686, 0.52996405, 0.07943855, 0.1358729 ,\n",
       "         0.034     , 0.976     , 0.        , 0.02005887, 0.3297553 ,\n",
       "         0.65018583, 0.        ],\n",
       "        [0.25540308, 0.01614291, 0.62010643, 0.99682732, 0.02847059,\n",
       "         0.04973651, 0.25161828, 0.04069905, 0.69846522, 0.2650274 ,\n",
       "         0.10621499, 0.22793645, 0.11608892, 0.72111518, 0.30576366,\n",
       "         0.26894142, 0.16110895, 0.05896701, 0.02608408, 0.05896701,\n",
       "         0.012     , 0.99      , 0.        , 0.09701467, 0.33574106,\n",
       "         0.56724427, 0.        ],\n",
       "        [0.91451086, 0.13705129, 0.52248482, 0.43045378, 0.05315114,\n",
       "         0.37285223, 0.17508627, 0.16383036, 0.57932425, 0.11815698,\n",
       "         0.19940776, 0.25161828, 0.7251195 , 0.68997448, 0.88795296,\n",
       "         0.5274723 , 0.73105858, 0.43045378, 0.04148712, 0.09621554,\n",
       "         0.013     , 0.987     , 0.        , 0.33333333, 0.33333333,\n",
       "         0.33333333, 0.        ],\n",
       "        [0.11304583, 0.12131884, 0.01746392, 0.04069905, 0.47751518,\n",
       "         0.01365366, 0.0511737 , 0.91606157, 0.05365665, 0.98634634,\n",
       "         0.78244978, 0.02188127, 0.02988668, 0.24787089, 0.06180347,\n",
       "         0.04973651, 0.15187116, 0.63876318, 0.11105597, 0.58419052,\n",
       "         0.012     , 0.988     , 0.        , 0.02005887, 0.3297553 ,\n",
       "         0.65018583, 0.        ]]), 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Validation\n",
    "with torch.set_grad_enabled(False):\n",
    "    for local_batch, local_labels in validation_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamDataset(Dataset):\n",
    "    #Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, feature_path, label_path):\n",
    "        'Initialization'\n",
    "        self.feature = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/' + ID + '.pt')\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
