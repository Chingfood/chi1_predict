{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialDataset(Dataset):\n",
    "    #Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, feature_path, label_path, name_list, channel=\"first\",transform = False):\n",
    "        'Initialization'\n",
    "        self.feature_path = feature_path\n",
    "        self.label_path = label_path\n",
    "        self.name_list = name_list\n",
    "        if (channel != \"first\"):\n",
    "            self.channel = \"last\"\n",
    "        else:\n",
    "            self.channel = channel\n",
    "        if transform == False:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = True\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return np.shape(self.name_list)[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        #ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        #X = torch.load('data/' + ID + '.pt')\n",
    "        \n",
    "        #y = self.labels[ID]\n",
    "        feature = np.load(os.path.join(self.feature_path,self.name_list[index]))\n",
    "        label = np.load(os.path.join(self.label_path,self.name_list[index]))\n",
    "        if self.transform == True:\n",
    "            feature = np.squeeze(feature,0)\n",
    "        if self.channel == \"last\":\n",
    "            feature = np.rollaxis(feature,-1,0)\n",
    "\n",
    "        return feature, label #, name_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# CUDA for PyTorch\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_size(kernel_size,dilation=1,stride=1,input_size = 1):\n",
    "    pad = ((input_size -1)*(stride-1) + dilation*(kernel_size -1))/2\n",
    "    if pad.is_integer():\n",
    "        return int(pad)\n",
    "    else:\n",
    "        raise NameError('value is not integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1, input_size = 1, padding = \"same\"):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    if padding == \"valid\":\n",
    "        pad = 0\n",
    "    else:\n",
    "        pad = padding_size(3,dilation,stride,input_size)\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, \n",
    "                     padding= pad, bias=False, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, num_filters, channels_in, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        # with kernel_size=1, max pooling is equivalent to identity mapping with stride\n",
    "        self.identity = nn.MaxPool2d(1, stride=stride)\n",
    "        self.num_zeros = num_filters - channels_in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.num_zeros))\n",
    "        out = self.identity(out)\n",
    "        return out\n",
    "\n",
    "# option B from paper\n",
    "class ConvProjection(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, channels_in, stride):\n",
    "        super(ResA, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels_in, num_filters, kernel_size=1, stride=stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "# experimental option C\n",
    "class AvgPoolPadding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, channels_in, stride):\n",
    "        super(AvgPoolPadding, self).__init__()\n",
    "        self.identity = nn.AvgPool2d(stride, stride=stride)\n",
    "        self.num_zeros = num_filters - channels_in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.num_zeros))\n",
    "        out = self.identity(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_filters, channels_in=None, dilation = 1, stride=1, res_option='B', use_dropout=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # uses 1x1 convolutions for downsampling\n",
    "        if not channels_in or channels_in == num_filters:\n",
    "            channels_in = num_filters\n",
    "            self.projection = None\n",
    "        else:\n",
    "            if res_option == 'A':\n",
    "                self.projection = IdentityPadding(num_filters, channels_in, stride)\n",
    "            elif res_option == 'B':\n",
    "                self.projection = ConvProjection(num_filters, channels_in, stride)\n",
    "            elif res_option == 'C':\n",
    "                self.projection = AvgPoolPadding(num_filters, channels_in, stride)\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.conv1 = conv3x3(channels_in, num_filters, dilation=dilation, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(channels_in, num_filters, dilation=dilation, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        if self.projection:\n",
    "            residual = self.projection(x)\n",
    "        out += residual\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n=61, res_option='B', use_dropout=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.res_option = res_option\n",
    "        self.use_dropout = use_dropout\n",
    "        self.conv1 = nn.Conv2d(526, 64, kernel_size=1, stride=1)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.layers1 = self._make_layer(n, 64, 64, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 3, kernel_size=1, stride=1)\n",
    "        self.norm2 = nn.BatchNorm2d(3)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv1d(3,3,kernel_size =1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "    def _make_layer(self, layer_count, channels, channels_in, stride):\n",
    "        return nn.Sequential(\n",
    "            ResBlock(channels, channels_in, stride, res_option=self.res_option, use_dropout=self.use_dropout),\n",
    "            *[ResBlock(channels,channels_in, 2**(n%5) ) for n in range(1,layer_count)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.layers1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = (torch.mean(out,2) + torch.mean(out,3))*0.5\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yPred,yTrue):\n",
    "\n",
    "    yPred= torch.clamp(yPred, 1e-6, (1. - 1e-6))\n",
    "    mask= (yTrue <= 2).squeeze_(0)\n",
    "    \n",
    "    return torch.nn.CrossEntropyLoss()(yPred[:,:,mask],yTrue[:,mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(526, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (layers1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (norm2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv1d(3, 3, kernel_size=(1,), stride=(1,))\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = ResNet(n=6)\n",
    "# defining the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     criterion = criterion.cuda()\n",
    "if use_cuda:\n",
    "    model.to(device)\n",
    "    #criterion.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()    # Training\n",
    "    for idx, (local_batch, local_labels) in enumerate(training_generator):\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(local_batch.float())\n",
    "        loss_train = customLoss(output_train, local_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss = loss_train.item()\n",
    "        if (index%4000 == 0):\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : 1.0612168312072754\n",
      "Epoch :  1 \t loss : 1.0500996112823486\n",
      "Epoch :  1 \t loss : 1.0869042873382568\n",
      "Epoch :  1 \t loss : 1.05828857421875\n",
      "Epoch :  1 \t loss : 1.0098966360092163\n",
      "Epoch :  1 \t loss : 1.0144140720367432\n",
      "Epoch :  1 \t loss : 0.9899494051933289\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n",
      "Epoch :  1 \t loss : nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4e1babb28015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Loop over epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-6b1539915da9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 2\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_list = \"/oasis/scratch/comet/qingyliu/temp_project/testing_name_list.npy\"\n",
    "test_feature_path = \"/oasis/scratch/comet/qingyliu/temp_project/conv_testing_input\"\n",
    "test_label_path = \"/oasis/scratch/comet/qingyliu/temp_project/chi1_label\"\n",
    "test_name_list = np.load(test_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = TrialDataset(test_feature_path,test_label_path,test_name_list,\"last\",True)\n",
    "# Parameters\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 1}\n",
    "testing_generator = torch.utils.data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Validation\n",
    "with torch.set_grad_enabled(False):\n",
    "    model.eval()\n",
    "    for local_batch, local_labels in validation_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        output_train = model(local_batch.float())\n",
    "        loss_train = customLoss(output_train, local_labels)\n",
    "        tr_loss = loss_train.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
